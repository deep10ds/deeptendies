{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      c     h       l     o   s           t        v       wma         ts  \\\n",
      "0  4.19  4.29  4.1300  4.13  ok  1594771200  1474126  0.000000 2020-07-15   \n",
      "1  4.17  4.20  4.0900  4.19  ok  1594857600  1330461  0.000000 2020-07-16   \n",
      "2  3.96  4.23  3.9381  4.16  ok  1594944000  3066549  4.068333 2020-07-17   \n",
      "3  3.85  4.06  3.7700  3.95  ok  1595203200  3401088  3.940000 2020-07-20   \n",
      "4  4.01  4.09  3.8800  3.90  ok  1595289600  3341012  3.948333 2020-07-21   \n",
      "\n",
      "         date       c_dji       h_dji       l_dji       o_dji s_dji  \\\n",
      "0  2020-07-15  26870.0996  27071.3301  26692.4805  27009.8105    ok   \n",
      "1  2020-07-16  26734.7109  26879.1602  26590.0098  26746.5703    ok   \n",
      "2  2020-07-17  26671.9492  26808.4297  26619.8809  26774.6191    ok   \n",
      "3  2020-07-20  26680.8691  26765.0195  26504.1992  26660.2891    ok   \n",
      "4  2020-07-21  26840.4004  27025.3809  26766.2207  26833.1406    ok   \n",
      "\n",
      "        t_dji      v_dji       wma_dji              ts_dji    date_dji  \n",
      "0  1594819800  384900000      0.000000 2020-07-15 13:30:00  2020-07-15  \n",
      "1  1594906200  287330000      0.000000 2020-07-16 13:30:00  2020-07-16  \n",
      "2  1594992600  296460000  26725.894833 2020-07-17 13:30:00  2020-07-17  \n",
      "3  1595251800  309390000  26686.869433 2020-07-20 13:30:00  2020-07-20  \n",
      "4  1595338200  364930000  26759.148100 2020-07-21 13:30:00  2020-07-21  \n",
      "<bound method NDFrame.head of           c       h         l       o   s           t         v         wma  \\\n",
      "0      4.19    4.29    4.1300    4.13  ok  1594771200   1474126    0.000000   \n",
      "1      4.17    4.20    4.0900    4.19  ok  1594857600   1330461    0.000000   \n",
      "2      3.96    4.23    3.9381    4.16  ok  1594944000   3066549    4.068333   \n",
      "3      3.85    4.06    3.7700    3.95  ok  1595203200   3401088    3.940000   \n",
      "4      4.01    4.09    3.8800    3.90  ok  1595289600   3341012    3.948333   \n",
      "..      ...     ...       ...     ...  ..         ...       ...         ...   \n",
      "167  220.14  283.00  206.0000  277.52  ok  1615766400  24226187  241.570000   \n",
      "168  208.17  220.70  172.3500  203.16  ok  1615852800  35422867  221.548333   \n",
      "169  209.81  231.47  204.0000  217.84  ok  1615939200  16481589  210.985000   \n",
      "170  201.75  218.88  195.6500  214.00  ok  1616025600  11799914  205.506667   \n",
      "171  200.27  227.00  182.6600  195.73  ok  1616112000  24552100  202.353333   \n",
      "\n",
      "            ts        date  ...  next_7_high  next_7_low  last_1_high  \\\n",
      "0   2020-07-15  2020-07-15  ...        4.305        3.77         0.00   \n",
      "1   2020-07-16  2020-07-16  ...        4.305        3.77         4.29   \n",
      "2   2020-07-17  2020-07-17  ...        4.305        3.77         4.20   \n",
      "3   2020-07-20  2020-07-20  ...        4.305        3.88         4.23   \n",
      "4   2020-07-21  2020-07-21  ...        4.305        3.92         4.06   \n",
      "..         ...         ...  ...          ...         ...          ...   \n",
      "167 2021-03-15  2021-03-15  ...        0.000        0.00       295.50   \n",
      "168 2021-03-16  2021-03-16  ...        0.000        0.00       283.00   \n",
      "169 2021-03-17  2021-03-17  ...        0.000        0.00       220.70   \n",
      "170 2021-03-18  2021-03-18  ...        0.000        0.00       231.47   \n",
      "171 2021-03-19  2021-03-19  ...        0.000        0.00       218.88   \n",
      "\n",
      "     last_1_low last_3_high  last_3_low  last_5_high  last_5_low last_7_high  \\\n",
      "0        0.0000         0.0        0.00       0.0000        0.00      0.0000   \n",
      "1        4.1300         0.0        0.00       0.0000        0.00      0.0000   \n",
      "2        4.0900         0.0        0.00       0.0000        0.00      0.0000   \n",
      "3        3.9381         0.0        0.00       0.0000        0.00      0.0000   \n",
      "4        3.7700         0.0        0.00       0.0000        0.00      0.0000   \n",
      "..          ...         ...         ...          ...         ...         ...   \n",
      "167    262.2700       348.5      146.10     210.8699      112.20    184.6800   \n",
      "168    206.0000       348.5      172.00     249.8500      113.12    184.6800   \n",
      "169    172.3500       348.5      172.00     348.5000      115.30    210.8699   \n",
      "170    204.0000       295.5      206.00     348.5000      127.50    249.8500   \n",
      "171    195.6500       295.5      172.35     348.5000      146.10    348.5000   \n",
      "\n",
      "    last_7_low  \n",
      "0         0.00  \n",
      "1         0.00  \n",
      "2         0.00  \n",
      "3         0.00  \n",
      "4         0.00  \n",
      "..         ...  \n",
      "167      44.70  \n",
      "168      86.00  \n",
      "169      86.00  \n",
      "170      99.97  \n",
      "171     112.20  \n",
      "\n",
      "[172 rows x 41 columns]>\n",
      "(172, 41)\n"
     ]
    }
   ],
   "source": [
    "# -- general imports --\n",
    "import yaml\n",
    "import pandas as pd\n",
    "# -- base configs of importing from deeptendies --\n",
    "from deeptendies.stonks import *\n",
    "from deeptendies.utils import generate_time_fields\n",
    "\n",
    "# just an example, use generated key from https://finnhub.io/dashboard\n",
    "# finnhub_token = \"c1c318v48v6sp0s58ffg\"\n",
    "\n",
    "# load secrets from yaml example:\n",
    "credentials='/home/stan/github/mltrade/secrets.yaml'\n",
    "with open(credentials) as credentials:\n",
    "    credentials = yaml.safe_load(credentials)\n",
    "    finnhub_token=credentials['finnhub-apikey']\n",
    "\n",
    "stock_sym='GME'\n",
    "days_ago=250\n",
    "start='2020-12-01'\n",
    "metrics_interested=['next_3_high', 'next_3_low']\n",
    "\n",
    "# get df from finnhub\n",
    "df = pd.DataFrame.from_dict(get_stock_data(stock_sym, days_ago, 'D', finnhub_token))\n",
    "generate_time_fields(df)\n",
    "time.sleep(0.2)\n",
    "\n",
    "# get df with added enriched data, right now only supports daily value\n",
    "df= get_enriched_stock_data(df, \"^DJI\", days_ago, 'D', finnhub_token)\n",
    "print(df.head())\n",
    "\n",
    "# feature engineering, calendar and ma, vwap\n",
    "df_proc = get_calendar_features(df)\n",
    "df_proc = get_moving_average(df)\n",
    "df_proc.fillna(method='backfill')\n",
    "df_proc = add_vwap_col(df)\n",
    "\n",
    "# feature engineering, get high and get low\n",
    "days=[1,3,5,7]\n",
    "df_new = get_high(df, days)\n",
    "df_new = get_low(df, days)\n",
    "print(df.head)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metric_interested = 'next_3_low'\n",
    "df[df[metric_interested].eq(0)] = np.nan\n",
    "\n",
    "# plt_visual_raw(stock_sym, metric_interested, df)\n",
    "# Create a new dataframe with only the 'Close column\n",
    "data = df.filter([metric_interested])\n",
    "\n",
    "# Convert the dataframe to a numpy array\n",
    "dataset = data.values\n",
    "\n",
    "# Get the number of rows to train the model on\n",
    "training_data_len = int(np.ceil( len(dataset) * .95 ))\n",
    "\n",
    "# print(\"training_data_len: %s\" %training_data_len )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.        , 0.        , 0.        , 0.00054394, 0.00074173,\n",
      "       0.00089008, 0.00074173, 0.00074173, 0.00074173, 0.00074173,\n",
      "       0.00098897, 0.00108787, 0.00111259, 0.00178015, 0.00143401,\n",
      "       0.00143401, 0.00143401, 0.00202739, 0.00281857, 0.00291747,\n",
      "       0.00359294, 0.00328834, 0.00328834, 0.00328834, 0.0036592 ,\n",
      "       0.0036592 , 0.00390644, 0.00390644, 0.0054888 , 0.0056861 ,\n",
      "       0.00618108, 0.00714533, 0.00949414, 0.01483459, 0.01651585,\n",
      "       0.01681254, 0.01681254, 0.01196657, 0.01038422, 0.01038422,\n",
      "       0.01038422, 0.01315334, 0.01441428, 0.01612026, 0.02279583,\n",
      "       0.02279583, 0.02279583, 0.02591109, 0.02591109, 0.02591109,\n",
      "       0.02635613, 0.02858132, 0.0292736 , 0.0273451 , 0.02709885,\n",
      "       0.02635613, 0.02635613, 0.02635613, 0.02669139, 0.02680117])]\n",
      "[0.03772931810314988]\n",
      "\n",
      "[array([0.        , 0.        , 0.        , 0.00054394, 0.00074173,\n",
      "       0.00089008, 0.00074173, 0.00074173, 0.00074173, 0.00074173,\n",
      "       0.00098897, 0.00108787, 0.00111259, 0.00178015, 0.00143401,\n",
      "       0.00143401, 0.00143401, 0.00202739, 0.00281857, 0.00291747,\n",
      "       0.00359294, 0.00328834, 0.00328834, 0.00328834, 0.0036592 ,\n",
      "       0.0036592 , 0.00390644, 0.00390644, 0.0054888 , 0.0056861 ,\n",
      "       0.00618108, 0.00714533, 0.00949414, 0.01483459, 0.01651585,\n",
      "       0.01681254, 0.01681254, 0.01196657, 0.01038422, 0.01038422,\n",
      "       0.01038422, 0.01315334, 0.01441428, 0.01612026, 0.02279583,\n",
      "       0.02279583, 0.02279583, 0.02591109, 0.02591109, 0.02591109,\n",
      "       0.02635613, 0.02858132, 0.0292736 , 0.0273451 , 0.02709885,\n",
      "       0.02635613, 0.02635613, 0.02635613, 0.02669139, 0.02680117]), array([0.        , 0.        , 0.00054394, 0.00074173, 0.00089008,\n",
      "       0.00074173, 0.00074173, 0.00074173, 0.00074173, 0.00098897,\n",
      "       0.00108787, 0.00111259, 0.00178015, 0.00143401, 0.00143401,\n",
      "       0.00143401, 0.00202739, 0.00281857, 0.00291747, 0.00359294,\n",
      "       0.00328834, 0.00328834, 0.00328834, 0.0036592 , 0.0036592 ,\n",
      "       0.00390644, 0.00390644, 0.0054888 , 0.0056861 , 0.00618108,\n",
      "       0.00714533, 0.00949414, 0.01483459, 0.01651585, 0.01681254,\n",
      "       0.01681254, 0.01196657, 0.01038422, 0.01038422, 0.01038422,\n",
      "       0.01315334, 0.01441428, 0.01612026, 0.02279583, 0.02279583,\n",
      "       0.02279583, 0.02591109, 0.02591109, 0.02591109, 0.02635613,\n",
      "       0.02858132, 0.0292736 , 0.0273451 , 0.02709885, 0.02635613,\n",
      "       0.02635613, 0.02635613, 0.02669139, 0.02680117, 0.03772932])]\n",
      "[0.03772931810314988, 0.03772931810314988]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#scaling\n",
    "# Scale the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_data = scaler.fit_transform(dataset)\n",
    "# scaled_data\n",
    "# Create the training data set\n",
    "# Create the scaled training data set\n",
    "train_data = scaled_data[0:int(training_data_len), :]\n",
    "# Split the data into x_train and y_train data sets\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(60, len(train_data)):\n",
    "    x_train.append(train_data[i - 60:i, 0])\n",
    "    y_train.append(train_data[i, 0])\n",
    "    if i <= 61:\n",
    "        print(x_train)\n",
    "        print(y_train)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "104/104 [==============================] - 6s 30ms/step - loss: 0.0212\n",
      "Epoch 2/20\n",
      "104/104 [==============================] - 3s 29ms/step - loss: 0.0096\n",
      "Epoch 3/20\n",
      "104/104 [==============================] - 3s 29ms/step - loss: 0.0051\n",
      "Epoch 4/20\n",
      "104/104 [==============================] - 3s 29ms/step - loss: 0.0066\n",
      "Epoch 5/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.0112\n",
      "Epoch 6/20\n",
      "104/104 [==============================] - 3s 29ms/step - loss: 0.0072\n",
      "Epoch 7/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.0063\n",
      "Epoch 8/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.0050\n",
      "Epoch 9/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.0044\n",
      "Epoch 10/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.0028\n",
      "Epoch 11/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.0030\n",
      "Epoch 12/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.0027\n",
      "Epoch 13/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.0037\n",
      "Epoch 14/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.0019\n",
      "Epoch 15/20\n",
      "104/104 [==============================] - 3s 29ms/step - loss: 0.0024\n",
      "Epoch 16/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.0028\n",
      "Epoch 17/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.0032\n",
      "Epoch 18/20\n",
      "104/104 [==============================] - 3s 29ms/step - loss: 0.0026\n",
      "Epoch 19/20\n",
      "104/104 [==============================] - 3s 29ms/step - loss: 0.0015\n",
      "Epoch 20/20\n",
      "104/104 [==============================] - 3s 30ms/step - loss: 0.0023\n",
      "[[0.92542017]\n",
      " [1.133013  ]\n",
      " [0.9001262 ]\n",
      " [0.9272224 ]\n",
      " [0.92792666]\n",
      " [0.98765486]\n",
      " [       nan]\n",
      " [       nan]]\n",
      "[[190.91772]\n",
      " [232.89923]\n",
      " [185.80252]\n",
      " [191.28218]\n",
      " [191.4246 ]\n",
      " [203.50343]\n",
      " [      nan]\n",
      " [      nan]]\n",
      "rmse nan\n",
      "     next_3_low  Predictions\n",
      "164      206.00   190.917725\n",
      "165      172.35   232.899231\n",
      "166      172.35   185.802521\n",
      "167      172.35   191.282181\n",
      "168      182.66   191.424606\n",
      "169         NaN   203.503433\n",
      "170         NaN          NaN\n",
      "171         NaN          NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-c98a0c4292a7>:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid['Predictions'] = predictions\n"
     ]
    }
   ],
   "source": [
    "# Convert the x_train and y_train to numpy arrays\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "# Reshape the data\n",
    "x_train = np.atleast_2d(x_train) # experimenting to solve the tuple index out of range issue\n",
    "\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "# print(\"x_train.shape:\")\n",
    "# print(x_train.shape)\n",
    "\n",
    "\n",
    "#LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Masking\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0., input_shape=(x_train.shape[1], 1))) # handle nans https://stackoverflow.com/questions/52570199/multivariate-lstm-with-missing-values\n",
    "model.add(LSTM(128, return_sequences=True, input_shape= (x_train.shape[1], 1)))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=1, epochs=20)\n",
    "\n",
    "# Test\n",
    "# Create the testing data set\n",
    "# Create a new array containing scaled values from index 1543 to 2002\n",
    "\n",
    "# training and validating\n",
    "test_data = scaled_data[training_data_len - 60:, :]\n",
    "\n",
    "# Create the data sets x_test and y_test\n",
    "x_test = []\n",
    "y_test = dataset[training_data_len:, :]\n",
    "for i in range(60, len(test_data)):\n",
    "    x_test.append(test_data[i - 60:i, 0])\n",
    "\n",
    "# Convert the data to a numpy array\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# Reshape the data\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# Get the models predicted price values\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "# Get the root mean squared error (RMSE)\n",
    "rmse = np.sqrt(np.mean(((predictions - y_test) ** 2)))\n",
    "print(\"rmse %s\" %rmse)\n",
    "\n",
    "## Plot the data Again\n",
    "# Plot the data\n",
    "train = data[:training_data_len]\n",
    "valid = data[training_data_len:]\n",
    "valid['Predictions'] = predictions\n",
    "\n",
    "# plot_predicted()\n",
    "\n",
    "print(valid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
